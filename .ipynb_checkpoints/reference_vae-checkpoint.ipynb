{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19e4ba6-e484-4c98-b248-7cb6a10f00f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ray-lightning/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/ray-lightning/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "import monai\n",
    "from losses import *\n",
    "\n",
    "bs = 128\n",
    "transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((32,32)),\n",
    "            ]\n",
    "        )\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms, download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms, download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd37598-65eb-42a8-912a-15cf4f1d28da",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"lr\": 0.00001,\n",
    "        \"latent_dim\": 256,\n",
    "        \"kernel_size\": 3,\n",
    "        \"dropout_rate\": 0.1,\n",
    "        \"alpha\": 1,\n",
    "        \"beta\": 0.01,\n",
    "        \"norm\": Norm.INSTANCE,\n",
    "         \"batch_size\": 256,\n",
    "        \n",
    "            \"val\": 3,\n",
    "            \"channel\": (32, 64, 64),\n",
    "            \"stride\": (1, 2, 4),\n",
    "            # \"resnet_units_batch\" : hp.choice(\"res6\", res_d6),\n",
    "\n",
    "        \"num_resnets\":  0,\n",
    "           \n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee11ce39-0163-40b8-862c-09f13628a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = monai.networks.nets.VarAutoEncoder(\n",
    "    dimensions=2,  \n",
    "    kernel_size=config[\"kernel_size\"],\n",
    "    in_shape=[1, 32,32],\n",
    "    out_channels=1,\n",
    "    channels=config[\"channel\"],\n",
    "    strides=config[\"stride\"],\n",
    "    latent_size=config[\"latent_dim\"],\n",
    "    norm=config[\"norm\"],\n",
    "    dropout=config[\"dropout_rate\"],\n",
    "    num_res_units=config[\"num_resnets\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2dc23bb-18ed-4e33-9ccb-57dcee18104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4075153-540f-468d-aa19-da7d0096c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "dice = Dice()\n",
    "loss_function = KLLoss(alpha=config[\"alpha\"], beta=config[\"beta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c72071-463c-4d82-8152-1859cbba96ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.0176\t Dice 0.0018\t KL 0.0015\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.0044\t Dice 0.0072\t KL 0.0141\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.0038\t Dice 0.0077\t KL 0.0147\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.0037\t Dice 0.0078\t KL 0.0145\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.0038\t Dice 0.0076\t KL 0.0143\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.0036\t Dice 0.0078\t KL 0.0147\n",
      "====> Epoch: 0 Average loss: 0.0043, mean Dice: 0.0074, mean KL: 0.0142\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.0036\t Dice 0.0078\t KL 0.0145\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.0037\t Dice 0.0078\t KL 0.0142\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.0036\t Dice 0.0078\t KL 0.0138\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.0035\t Dice 0.0078\t KL 0.0139\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.0037\t Dice 0.0078\t KL 0.0142\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.0035\t Dice 0.0079\t KL 0.0137\n",
      "====> Epoch: 1 Average loss: 0.0036, mean Dice: 0.0078, mean KL: 0.0140\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.0035\t Dice 0.0079\t KL 0.0139\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.0035\t Dice 0.0079\t KL 0.0137\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.0035\t Dice 0.0079\t KL 0.0138\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.0035\t Dice 0.0079\t KL 0.0135\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.0036\t Dice 0.0078\t KL 0.0133\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.0035\t Dice 0.0079\t KL 0.0133\n",
      "====> Epoch: 2 Average loss: 0.0035, mean Dice: 0.0079, mean KL: 0.0134\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.0034\t Dice 0.0079\t KL 0.0131\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.0034\t Dice 0.0079\t KL 0.0128\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.0035\t Dice 0.0079\t KL 0.0126\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.0036\t Dice 0.0078\t KL 0.0129\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.0035\t Dice 0.0078\t KL 0.0125\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.0035\t Dice 0.0079\t KL 0.0123\n",
      "====> Epoch: 3 Average loss: 0.0035, mean Dice: 0.0079, mean KL: 0.0127\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.0036\t Dice 0.0079\t KL 0.0127\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.0034\t Dice 0.0079\t KL 0.0124\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.0034\t Dice 0.0079\t KL 0.0123\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.0034\t Dice 0.0079\t KL 0.0120\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.0034\t Dice 0.0079\t KL 0.0120\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.0035\t Dice 0.0079\t KL 0.0116\n",
      "====> Epoch: 4 Average loss: 0.0035, mean Dice: 0.0079, mean KL: 0.0120\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_dice = 0\n",
    "    train_kl = 0\n",
    "    \n",
    "    \n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.cuda()\n",
    "        y = x.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        #out is a tuple of (recon_batch, mu, logvar, z)\n",
    "        out = model(x)\n",
    "        loss, kl = loss_function(out, y)\n",
    "        recon_batch = out[0]\n",
    "        loss.backward()\n",
    "        batch_dice = dice(recon_batch, y)\n",
    "        train_loss += loss.item()\n",
    "        train_dice += batch_dice.item()\n",
    "        train_kl += kl.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}\\t Dice {:.4f}\\t KL {:.4f}'.format(\n",
    "                epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(x), batch_dice.item() / len(x), kl.item()*config[\"latent_dim\"] / len(x)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}, mean Dice: {:.4f}, mean KL: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset), train_dice / len(train_loader.dataset), \n",
    "    train_kl*config[\"latent_dim\"]/len(train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65493a6e-7300-45c9-915d-a661124e8b40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-ray-lightning-py",
   "name": "pytorch-gpu.1-10.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m90"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ray-lightning]",
   "language": "python",
   "name": "conda-env-ray-lightning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
